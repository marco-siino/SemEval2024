{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/SemEval2024/blob/main/Task%2010/SemEval_2024_Task_10_Mistral7B_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubSCVmmlBKX"
      },
      "source": [
        "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL68Itp9Bm-",
        "outputId": "b86722ec-8cfc-49f2-a83d-2cc09848941e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python>=0.1.79\n",
            "  Downloading llama_cpp_python-0.2.38.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m239.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m337.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python>=0.1.79)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl size=9697774 sha256=66f7aa0443e03ca9c4b2249b9e2f199e48408f7c58db4a36f20fb12e7a0c25cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ze8r8m0s/wheels/eb/58/77/20d3d9a235b4930050fbcde1ad4f0a4d054644269e801b08aa\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.3\n",
            "    Uninstalling numpy-1.26.3:\n",
            "      Successfully uninstalled numpy-1.26.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.38\n",
            "    Uninstalling llama_cpp_python-0.2.38:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.38\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.38 numpy-1.26.3 typing-extensions-4.9.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from deep_translator import GoogleTranslator\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ns57iDlBKa"
      },
      "source": [
        "Downloading an instruction-finetuned Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uDMqQmBfAhYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7eff0b-fe0c-44a8-dcd3-ec6fefa1b828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
        "# you might need to tweak with respect to your hardware.\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4, #16, # CPU cores\n",
        "    n_batch=800, #8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=8192, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset for the three subtasks."
      ],
      "metadata": {
        "id": "jeXgLOpd4ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtask 1.\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_train_erc.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_val_erc.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_test_erc.json'\n",
        "\n",
        "train_path_1 = 'MaSaC_train_erc.json'\n",
        "val_path_1 = 'MaSaC_val_erc.json'\n",
        "test_path_1 = 'MaSaC_test_erc.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKaC_tQ-2xbI",
        "outputId": "a25db731-986e-4d6d-faf4-cc88fe0bbdf4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-03 15:27:13--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_train_erc.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 720262 (703K) [text/plain]\n",
            "Saving to: ‘MaSaC_train_erc.json.2’\n",
            "\n",
            "MaSaC_train_erc.jso 100%[===================>] 703.38K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-03 15:27:14 (15.7 MB/s) - ‘MaSaC_train_erc.json.2’ saved [720262/720262]\n",
            "\n",
            "--2024-02-03 15:27:14--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_val_erc.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109206 (107K) [text/plain]\n",
            "Saving to: ‘MaSaC_val_erc.json.2’\n",
            "\n",
            "MaSaC_val_erc.json. 100%[===================>] 106.65K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-03 15:27:14 (5.23 MB/s) - ‘MaSaC_val_erc.json.2’ saved [109206/109206]\n",
            "\n",
            "--2024-02-03 15:27:14--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask1/MaSaC_test_erc.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114548 (112K) [text/plain]\n",
            "Saving to: ‘MaSaC_test_erc.json.2’\n",
            "\n",
            "MaSaC_test_erc.json 100%[===================>] 111.86K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-03 15:27:14 (4.87 MB/s) - ‘MaSaC_test_erc.json.2’ saved [114548/114548]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtask 2.\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_train_efr.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_val_efr.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_test_efr.json'\n",
        "\n",
        "train_path_2 = 'MaSaC_train_efr.json'\n",
        "val_path_2 = 'MaSaC_val_efr.json'\n",
        "test_path_2 = 'MaSaC_test_efr.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJjFfOQx49oO",
        "outputId": "bab94d09-1053-4191-e8f2-cbeed2d8155a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-03 15:27:14--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_train_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8644822 (8.2M) [text/plain]\n",
            "Saving to: ‘MaSaC_train_efr.json.2’\n",
            "\n",
            "MaSaC_train_efr.jso 100%[===================>]   8.24M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-03 15:27:14 (92.2 MB/s) - ‘MaSaC_train_efr.json.2’ saved [8644822/8644822]\n",
            "\n",
            "--2024-02-03 15:27:15--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_val_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 666651 (651K) [text/plain]\n",
            "Saving to: ‘MaSaC_val_efr.json.2’\n",
            "\n",
            "MaSaC_val_efr.json. 100%[===================>] 651.03K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-03 15:27:15 (14.6 MB/s) - ‘MaSaC_val_efr.json.2’ saved [666651/666651]\n",
            "\n",
            "--2024-02-03 15:27:15--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask2/MaSaC_test_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 645559 (630K) [text/plain]\n",
            "Saving to: ‘MaSaC_test_efr.json.2’\n",
            "\n",
            "MaSaC_test_efr.json 100%[===================>] 630.43K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-03 15:27:15 (13.9 MB/s) - ‘MaSaC_test_efr.json.2’ saved [645559/645559]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtask 3.\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_train_efr.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_val_efr.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_test_efr.json'\n",
        "\n",
        "train_path_3 = 'MELD_train_efr.json'\n",
        "val_path_3 = 'MELD_val_efr.json'\n",
        "test_path_3 = 'MELD_test_efr.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cacTaDUo4-TN",
        "outputId": "77fe5c82-aa8c-45a7-c789-9dc4ff33a134"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-03 15:27:15--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_train_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5180775 (4.9M) [text/plain]\n",
            "Saving to: ‘MELD_train_efr.json.2’\n",
            "\n",
            "MELD_train_efr.json 100%[===================>]   4.94M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-02-03 15:27:16 (68.4 MB/s) - ‘MELD_train_efr.json.2’ saved [5180775/5180775]\n",
            "\n",
            "--2024-02-03 15:27:16--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_val_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283366 (277K) [text/plain]\n",
            "Saving to: ‘MELD_val_efr.json.2’\n",
            "\n",
            "MELD_val_efr.json.2 100%[===================>] 276.72K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-02-03 15:27:16 (8.91 MB/s) - ‘MELD_val_efr.json.2’ saved [283366/283366]\n",
            "\n",
            "--2024-02-03 15:27:16--  https://raw.githubusercontent.com/marco-siino/SemEval2024/main/Task%2010/dataset/subtask3/MELD_test_efr.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661869 (646K) [text/plain]\n",
            "Saving to: ‘MELD_test_efr.json.2’\n",
            "\n",
            "MELD_test_efr.json. 100%[===================>] 646.36K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-03 15:27:16 (13.9 MB/s) - ‘MELD_test_efr.json.2’ saved [661869/661869]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create few-shot samples from training and validation set."
      ],
      "metadata": {
        "id": "fHPX3E0PCJ9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples_task1(json_set_path,nr_samples):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_counter = 0\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  with open(json_set_path, 'r') as istr:\n",
        "    train_1_json = json.load(istr)\n",
        "  for i in tqdm.trange(len(train_1_json)):\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "      break\n",
        "\n",
        "    for sentence_nr in range(0,len(train_1_json[i]['speakers'])):\n",
        "      few_shot_counter+=1\n",
        "      #print(train_1_json[i]['speakers'][sentence_nr])\n",
        "      translated_utterance = GoogleTranslator(source='hi', target='en').translate(train_1_json[i]['utterances'][sentence_nr])\n",
        "      few_shot_samples += train_1_json[i]['speakers'][sentence_nr]+' // ' + translated_utterance + ' // ' + train_1_json[i]['emotions'][sentence_nr] + \" \\n \"\n",
        "\n",
        "      if few_shot_counter == nr_few_shot_samples:\n",
        "        break\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "99ag8gav51mC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples = create_few_shot_samples_task1(train_path_1,20)\n",
        "few_shot_samples += create_few_shot_samples_task1(val_path_1,20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "5fb88c9249204a1e8821a780a0405c5b",
            "d93c51d56c4049fc92d3498897bd3f9b",
            "caf10d9705ed4f6e9823a2356024c4c5",
            "38fbcd9f2a524d90852171c10bf65917",
            "7d5d5e1b599540a78dc76f5a1266517f",
            "a07c423b64f249dda5b7e1df60fac339",
            "78d38b363f7c46ac94814956d7565a03",
            "f7edc3eef71e451883d3584f8c2f6215",
            "22e25257237d4f7e994384fc0e6788c6",
            "0edd43dd83ee4731873add73e5c2039a",
            "f94e6f30c89c412aa840139f285ea591",
            "52280b866d7245d99f5891e4a73e146b",
            "a784104413cf4a6aa1c8b51ba6e99fea",
            "f168f7ac54f44ead9fd3d1baa23316f6",
            "415bfee5ea7645e3864005ee2f40eb95",
            "551493ce6e0d484a86bf4a481ecc1bf6",
            "6107ef20cf304c0f8d72b125f7e35400",
            "86b3b3e6aaae45a4b374223573bc89b5",
            "1317cd19a340452fa46052e337fc57a4",
            "1c810d2e07474cfba53833152a45dff2",
            "a2d59f99dc52485ca69fc6a9526e2909",
            "ca4978f75aa446dfb19328a6a2d5c939"
          ]
        },
        "id": "o8IaDH-7GmrN",
        "outputId": "2099575a-5698-46b3-b903-9aa40ea49de9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/343 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fb88c9249204a1e8821a780a0405c5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52280b866d7245d99f5891e4a73e146b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygkCG2c1X8XG",
        "outputId": "4487d3a5-611f-4b30-db8f-147da8b0ff8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maya // What does Indravadan keep in the house? Indravadan please why didn't you throw away all the useless things? // disgust \n",
            " indu // ok, let's rise, let's spring // contempt \n",
            " rosesh // Momma! Hath Chodiye Dad! // anger \n",
            " indu // Look Maya or I am not speaking! Then you will say that you are not throwing the wrong things, come on. // neutral \n",
            " maya // Indravadan, for how many years have you been telling this joke? What is happening today? // neutral \n",
            " indu // joke on kaikai // contempt \n",
            " rosesh // very funny // contempt \n",
            " indu // Maya, Monisha and Sahil are here, see what they are using and see how much they have done. // neutral \n",
            " sahil // hi guys // joy \n",
            " rosesh // hi // joy \n",
            " good rosesh // What are you doing Rosesh? India is your country! Bhartiya is a true human being. It is better not to read anyone's letter. // neutral \n",
            " bad rosesh // hahaha read the book rosesh baby or read the book also. How nice looks the gel in the balloon, ding dong bell. // neutral \n",
            " good rosesh // Poetry is good but thought is not good, villain Rosesh. Momma said it is wrong to read someone else's letter. // neutral \n",
            " bad rosesh // It's wrong, but mom told you to brush twice and you only did it once, wasn't that wrong? // contempt \n",
            " good rosesh // galat tha per // neutral \n",
            " bad rosesh // Wasn't it wrong when momma asked you for green chatni and you passed red? // contempt \n",
            " good rosesh // wo to galat tha // neutral \n",
            " bad rosesh // Last year mom's new hair style was like an ostrich and you called her a nymph, wasn't that wrong? // contempt \n",
            " good rosesh // han tha tha tha! // neutral \n",
            " rosesh // Please please please, please log in with a sophisticated argument otherwise mom will say that I am very middle class. You decide then I will open this envelope and read it. // neutral \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect all the emotions in the train and in the dev set for subtask 1."
      ],
      "metadata": {
        "id": "G2tqKNMQa_We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_list = []\n",
        "\n",
        "# Train set first.\n",
        "with open(train_path_1, 'r') as istr:\n",
        "  file_json = json.load(istr)\n",
        "  for i in tqdm.trange(len(file_json)):\n",
        "    for sentence_nr in range(0,len(file_json[i]['emotions'])):\n",
        "      #print(file_json[i]['emotions'][sentence_nr])\n",
        "      if file_json[i]['emotions'][sentence_nr] not in emotions_list:\n",
        "        emotions_list.append(file_json[i]['emotions'][sentence_nr])\n",
        "      break\n",
        "\n",
        "# Validation set after.\n",
        "with open(val_path_1, 'r') as istr:\n",
        "  file_json = json.load(istr)\n",
        "  for i in tqdm.trange(len(file_json)):\n",
        "    for sentence_nr in range(0,len(file_json[i]['emotions'])):\n",
        "      #print(file_json[i]['emotions'][sentence_nr])\n",
        "      if file_json[i]['emotions'][sentence_nr] not in emotions_list:\n",
        "        emotions_list.append(file_json[i]['emotions'][sentence_nr])\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "b0b39542d0d94fef85de4ee76f15253b",
            "7c53c6e842534842800f5bf4a40cede6",
            "30ff0df7708d40a4b31403a62e3275b2",
            "269eeb288ec14bfb8783b27453a66984",
            "0a739823114e4d2a9fe788a092c0f8dc",
            "f1432c20d2224906b2f58eb3c7ad03a6",
            "c552d29fc9dc4559a7e94b89929a9f49",
            "6372844a138e499885e16433e6a1a54d",
            "e1fccf6a12a04fae8200babfc160f66a",
            "0212db505ad74b17a3dcfa40eb46a1a4",
            "597637adf57c456780b6d55460ef797c",
            "56426e3783e84b3a9ff130e2961e0dae",
            "671d225527d4482f9082c0e69edba200",
            "76db46c7390045fcbd28259f483462ce",
            "301194cc1abf4b0f8f699e3b9b442a30",
            "d57c43b2ea88417a99854d06a3b7831a",
            "b288cba450b94209bb56c6a47fcae8bb",
            "2bd368e3dbd8437c984df36a43dc2a23",
            "aff1289a5acc4786930ac433617c370d",
            "c101ef0afd1742daa4a9a0e7043dd80c",
            "ccc99f43992e45b8b69f095cddaa4965",
            "0f12db3e94724f39a1afbb021f909751"
          ]
        },
        "id": "6Par_biTZecL",
        "outputId": "7b427c60-8593-495d-a038-e027d985f2bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/343 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0b39542d0d94fef85de4ee76f15253b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56426e3783e84b3a9ff130e2961e0dae"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-BhYoqQaYDp",
        "outputId": "47af5697-0a25-4932-d11f-c65d2f1f97a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['disgust',\n",
              " 'joy',\n",
              " 'neutral',\n",
              " 'anger',\n",
              " 'sadness',\n",
              " 'contempt',\n",
              " 'surprise',\n",
              " 'fear']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run!"
      ],
      "metadata": {
        "id": "r6Cqo0JqotN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UKo1-X5OvT4b",
        "outputId": "dcb50364-f1e6-4840-b9e0-c19dfda78261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // kuchh karo sahil please kuchh karo. mera rosesh adopt ho karke chala gaya na to me, i know this sounds horribly melodramatic, monishaish, par me mar jaaungi. i swear mein mar jaaungi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // Please go to the beach. My rosesh started adopting me, I know this sounds horribly melodramatic, monishaish, but I will die. i swear i will live in the sea // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // monisha tum muskura kya rahi ho?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // monisha why are you smiling? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // haila! main? main kahan? kya daddy ji?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // Haila! main? where main? what daddy ji? // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // maya kuchh actually marne wali nahin hai. it's just an exaggeration, jaise main nahin kahta ki rosesh ko koi adopt kar lega to main khushi se mar jaunga, you know its like that.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Maya Kuchh is not actually going to die. it's just an exaggeration, like I don't say that if someone adopts Rose, he will die happily, you know its like that. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // stop it! stop it! stop it! tumhari isi tarah ki harkato ki vajah se mera baccha ghar chhodkar ja raha hai!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // stop it! stop it! stop it! It is because of your actions that my child is running away from home! // anger\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // maya mene kya kiya? wo to be or not to be ka letter to tum hi ne likha tha!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // what did Maya do to me? wo to be or not to be ka letter to tum hi ne likha tha! // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // han to tumne padha kyun nhi use kholke itne saal?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // Why don't you open it all the time? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // come on maya tumne us pe \"to my darling husband\" likha tha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // come on maya you wrote on us \"to my darling husband\" // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // to?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // to? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // to?! sahil tab ham shadishuda the. zara socho, shaadi ke bad 24 ghante sath rahane wali biwi agar tumhen khat mein likhti hai \"to my darling husband\" to tum khologe wo khat?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // to?! Sahil tab ham shadishuda the. Just think, if the wife who lives with you for 24 hours after marriage writes \"to my darling husband\" in your bed, will you find that bed? // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // sahil!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // shore! // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // monisha bechari aisa khat kabhi likhegi hi nahin na!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // Poor Monisha, you will never write such a letter! // sadness\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // han likhegi bhi kaise? uske ghar mein na writing paper hai, na pen. agar pen kahin dikh bhi jaaye to use kaan khujane ke liye istemal karti hai. kaan me jab infection ho jata to garam karke lehsun wala tel dalti hai. tel ke dhabbe kapdon per pade to use fenk dene ki bajaye ghar mein pehenti hai! aur kapde pehanne se pahle train mein kharida hua \"pushpanjali\" naam ka talcum powder use karti hai!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // How will you even write? There is no writing paper, no pen in his house. Even if the pen is seen somewhere, where is it used to scratch? When there is an infection in the ear, garlic oil is added to heat it. Oil stains on clothes are used at home instead of being thrown away! And before wearing clothes, she uses talcum powder named \"Pushpanjali\", bought in the train! // disgust\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // maya! maya monisha haye haye, baad me! nahi to wo rosesh adopt hoke chala jaega kahin!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Maya! maya monisha haye haye, baad me! Nahi to wo rosesh hoke hoke chala jaga kahin! // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // oh my god! sahil please.. please kuchh karo. mere bacche ko koi adopt karne aa gaya to kya karenge?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // oh my god! shore please..please go away. If anyone wants to adopt my child, what will they do? // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // kaise koi aayega mom? agar koi adopt karne ki sochega agar to bachcha adopt karega. rosesh ko kyon koi adopt karega?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // how will someone come mom? If anyone thinks of adopting then will adopt the child. Why would anyone adopt Rosesh? // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // maya is newspaper mein usne apni tasvir bhi di hai dekho.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Maya is in the newspaper, we have also given her picture, see. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // han lekin ismein photo mein vah kitna chhota baby hi lag raha hai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // yes but in the name of the photo he is looking so cute baby // joy\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // to koi use uthake 'ulelele' aisa karke khelega kya? uski size dekhi hai tumne?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Who will use 'ulelele' like this? Have you seen its size? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // dad is right mom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // dad is right mom. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // sahil, lekin rosesh bhaiya to amir ladke hain na, to unki daulat hadapne ke liye koi unhen adopt karne a gaya to?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // Sahil, but Rosesh bhaiya is not a rich boy, so someone should adopt him to inherit his wealth? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // she is right. she is absolutely right. oh my god!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // she is right. she is absolutely right. oh my god! // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // mom you need to relax. apna dushyant gaya hai na newspaper ke office mein.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // mom you need to relax. Your friend has gone to the newspaper office. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // dushyant?! dushyant us couple ke ghar ka television, fridge, mobile phone, yah sab theek chal raha hai ki nahin vah check karega!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // dushyant?! Dushyant is checking the television, fridge, mobile phone of our couple's house, all this is running smoothly. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // mummy ji theek kah rahi hain. yad hai? jab gunwan mama ji gujar gaye the plane crash mein, tab dushyant jijaji ne shok sabha mein aakar kya kaha tha?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // Where are you mummy? do you remember? When the poor mother passed away when the plane crashed, what did the poor brother say to the mourning gathering? // sadness\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // han yah sab kaise ho gaya mummy ji poochhne ki bajaye, \"kaun si airlines thi thi? aircraft kaun sa tha? boeing 747 yah airbus thi?\" aisa rote rote poochha tha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // So how did all this happen? Instead of asking mom, \"Which airlines did you have? What kind of aircraft was it? Boeing 747 or Airbus?\" he asked so bluntly // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // suna tumne. sahil kuchh karo please. pura chance hai ki koi fraud couple rosesh ko adopt kar jaega aur fir use torture karega!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // suna tune. Please go to the beach. There is full chance that some fraud couple will adopt Rosesh and then torture him! // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // aankhen fod ke bitha denge rosesh bhaiya ko traffic signal per bheekh mangne ke liye!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // You will see Rosesh Bhaiya with open eyes to beg for the traffic signal! // surprise\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // monisha!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // monisha! // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // sahil maine padha hai! kabhi kabhi to yah gunde badmash bacchon ko lula langda karke bheekh mangwane ke liye unse train mein gane gavate hain.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // I have reached the shore! Sometimes these goons make the children sit in the train to make them sleep and ask for alms. // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // haan haan haan, or fir, apne rosesh ki to awaaz bhi match karti hai un train mein gana gane wale beekh mangon se! \"dekha hai pahli... sajan ki aankhon mein pyar\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Haan haan haan, or then, the voice of your anger can also match the song in the train, only by asking the bees! \"I have seen for the first time... love in Sajan's eyes\" // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // ese down market gane gate hain log trains mein!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // This down market is going through log trains! // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: dushyant // hello everybody\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: dushyant // hello everybody // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // dushyant! kya hua dushyant kya kaha newspaper walon ne?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // dushyant! What happened to Dushyant and what did the newspaper people say? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: dushyant // kahenge kya? bolati band ho gai unki.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: dushyant // what to say? I will tell you the band. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // bolati band?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // bolati band? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: dushyant // han unke lift ke pankhe mein se awaaz a rahi hai vah oiling na karne ki vajah se nahin tha, balki pan masale ka packet andar ghusane ki wajah se aa rhi hai yah maine sabit kar diya tu aankhen phadke dekhte rahe ho log bilkul vaise hi jaise aap log dekh rahe hain abhi mujhe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: dushyant // The noise coming from the fan of their lift was not due to oiling, but due to the pan masala packet being rubbed, I have confirmed that you are looking with your eyes wide, people are exactly like that. you are watching the log right now // surprise\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indu // sahil damad ka murder apne indian law mein allowed hai na?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indu // Is murder of Sahil Damad allowed in our Indian law? // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // dushyant rosesh ki ad ke liye koi response aaya?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // Is there any response to Dushyant Rosesh's ad? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: dushyant // haan haan. wo newspaper walon se maine poochha tha ki bhai koi phone wone aya? to unhone kaha ki ek couple ka phone aaya tha aur poochha tha ki yah vahi rosesh hai jo natako mein kaam karta hai? vah \"dudh ki vyatha- chai banu ya bachde ka khurak?\" jismein vah dudh banaa tha. to newspaper walon ne haan kaha to us couple ne \"huu\" aise kaha aur phone rakh diya!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: dushyant // haan ​​haan. I asked the newspapermen, 'Bhai, which phone did you call?' He said that a couple had called and asked if this is the rose which helps in fights? That \"Will you make milk milk or baby food?\" That milk was made from it. The newspaper guys did not say anything to us. The couple said \"huu\" like that and hung up the phone! // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // indu tumne vah mere earplugs dekhe hain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // indu you have seen my earplugs // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // earplugs kyon?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // why earplugs? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // 11:30 baj rahe hain na madhubhai ki bhatiji ka sone ka time ho gaya hai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // 11:30 You are in the morning, isn't it Madhubhai's niece's bedtime? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // are baap re yyane announcement shuru ho jayegi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // are baap re yaane announcement will start // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: dvd player // train sound.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: dvd player // train sound. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // aaaa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // aaaa // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // happy anniversary to you too darling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // happy anniversary to you too darling // joy\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // thank you!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // thank you! // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // and here's your anniversary gift diamond bracelet ka dabba.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // And here's your anniversary gift diamond bracelet box. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // sirf dibba?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // just box? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // hold on hold on bracelet alag se hai. hey you are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // hold on hold on bracelet is different. hey you are // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // indu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // indu // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // oh sorry\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // oh sorry // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // aur ye meri taraf se gift\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // and this is a gift from my side // joy\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // kya hai ismein?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // what is ismein? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // meduvada sambar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // meduvada sambar // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // wedding anniversary gift ke liye medu vada sambhar monisha?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // Medu Vada Sambhar Monisha for wedding anniversary gift? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // rosesh tum kya tum kya gift lai ho\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // rosesh what gift have you brought? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // meri taraf se kal party mein. filhal ke liye.  tra tro troooo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // I was at the party yesterday. For now. tra tro trooooo // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // haha and that means?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // haha and that means? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // hathi ki bhasha mein happy wedding anniversary to my parents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // Happy wedding anniversary to my parents in elephant language. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // rosesh.aaaa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // rosesh.aaaa // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // matlab?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // matlab? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // matlab hathi ki bhasha mein just shut up.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // Meaning in elephant language just shut up. // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // hahaha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // hahaha // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // aila sab kuchh dekha hai na?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // Have you seen everything like this? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // aur 7 sal ke bad monisha vaisi ki vaisi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // and after 7 years monisha vaisi's vaisi // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: mr. sindolin // ok lights on karte hain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: mr. sindolin // ok lights are on the cards // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // aap to amazing mistic hai. mr. sindolin mujhe aap per ek kavita suj rahi hai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // You are an amazing mystic. mr. sindolin i am writing a poem for you // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // nai nai nai. rosesh abhi nahin. yaar main to samajhta tha tum kuchh chalu pocket maar ho. ya fir vah boli bali ladkiyon ko behla fusla kar rape karne wale koi paakhandi doongi baba ho. lekin tum to kamal ke nikale bhai hanhai na monisha. pasise vasool?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // nai nai nai. rosesh still not there. The main thing to understand is that you still have some pocket money. Or then she said that you are a hypocrite father who would strip the girls and rape them. But you turned out to be amazing brother, don't worry Monisha. paise vasool? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // ek minute ek minute yah hai kya i mean is it hypnosis yah mesmerism kya hai kya?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // ek minute ek minute yah kya hai i mean is it hypnosis yah mesmerism kya hai kya hai? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: mr. sindolin // the art of rosebinzaro numestikization philanthropic philosophy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: mr. sindolin // the art of rosebinzaro numestikization philanthropic philosophy // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // han vah to main samajh gaya. lekin yah hai kya? whatever it is i don't believe this\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // He went to the main society. but what is this? whatever it is i don't believe this // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // lekin ismein nahin believe karne ka kya hai sahil? tumne dekha nahin kaise dhup se khate hue magarmacch ki tarah samne padi hui thi tv ke samose khate khate. i bet you 7 sal bad samose bhi yahi honge. woh manisha hi thi there is no trick\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // But if I don't believe then what is the point? Haven't you seen how he eats his samosas while sitting in front of the TV like a crocodile eating his food? I bet you 7 years later samosa will also be here. woh manisha hi thi there is no trick // disgust\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // han sahil. ham donon hi the. 8 sal bad bhi sirf ham donon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // han shore. ham donon hi the. Even after 8 years, we are the only ones. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // nahin nahin monisha woh baccha niche khelne gaya hoga na. isiliye najar nahin aaya. hai na?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // Well, Monisha, that child will play the niche. That's why I was not visible. isn't it? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // koi baccha nahin tha daddyji\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // which child is not there daddyji // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // monisha come on now don't be upset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // monisha come on now don't be upset // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // kya don't be upset sahil pata hai kal hi main ragada baba ki tekari per 60 rupaye 50 paise chadaaye the. bacche ki mannat ke liye. aur ragada baba to mannat puri na hone per refund bhi nahin dete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // kya don't be upset sahil pata hai kal hi main ragada baba ki tekari per 60 rupaye 50 paise chadaaye the. bacche ki mannat ke liye. aur ragada baba to mannat puri na hone per refund bhi nahin dete. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // matlab aapko 60 rupaye 50 paise ke liye afsos ho raha hai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // It means you are feeling sorry for 60 rupees and 50 paise. // sadness\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: indravardhan // are kyon ho afsos bhai kyon ho bhai vah paise to puri tarah vasul hone wale hain hai na sahil?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: indravardhan // Why are you sorry brother? Why are you sorry? That money has to be collected completely, isn't it? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: sahil // han bilkul obviously mr. sindolin ab ham dekhi rahe hai to dekhi na is ghar mein bacche ki jhalak dikhai de. saat sal bad to baccha khel raha hoga\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: sahil // han absolutely obviously mr. Sindolin, now we are watching to see that the child's face is visible in the house. the child will be playing at the right time // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: maya // absolutely\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: maya // completely // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: monisha // haha dikhaiye na dikhaiye na. han lekin bacche ka rang na gora hona chahie. bhuri bhuri aankhen aur uske bal jo hai unko ghane rakhna na aur uske pal ke na aisi lambi lambi aur uski honth na ekadam lal lal seb jaise\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: monisha // Haha, let's show it. Yes but the child's complexion should not be fair. Her eyes are brown and her hair is thick and her lips are not long like this and her lips are like red apples. // surprise\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: theif // abhi 2 minut deta hun han. tu gehne lekar varna teri sans ka gala kar dunga\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: theif // I just gave you 2 minutes. You need a doctor or else your breath will be choked. // disgust\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: madhu // hain?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: madhu // hain? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: theif // hain kya hain. main isase 2 minut diye inhone gana nahin le to main inka gala kar dunga \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: theif // what is it? Main isase 2 minutes diye he nahin le to main inka throat kar dunga // fear\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: madhu // hain?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: madhu // hain? // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: rosesh // inko sunai nahin deta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: rosesh // I did not hear anything. // neutral\n",
            "\n",
            "\n",
            "\n",
            "ORIGINAL: theif // are yaar sunai nahin deta lekin dikhai to deta hai. aa gale mein chaku rakha dikhai nahin deta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-30d6a58e4be7>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m#print(prompt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       response = lcpp_llm(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mResponse\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \"\"\"\n\u001b[0;32m-> 1429\u001b[0;31m         return self.create_completion(\n\u001b[0m\u001b[1;32m   1430\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m    889\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKV\u001b[0m \u001b[0mslot\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mtry\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     < 0 - error\"\"\"\n\u001b[0;32m-> 1622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllama_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with open('answer.txt', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "# simple JSON loading\n",
        "with open(test_path_1, 'r') as istr:\n",
        "    data_val_all = json.load(istr)\n",
        "num_sample = len(data_val_all)\n",
        "#print(num_sample)\n",
        "\n",
        "\n",
        "with open(test_path_1, 'r') as istr:\n",
        "    test_1_json = json.load(istr)\n",
        "for i in range(0,len(test_1_json)):\n",
        "  current_sample =''\n",
        "  for sentence_nr in range(0,len(test_1_json[i]['speakers'])):\n",
        "      print(\"\\n\\n\")\n",
        "      print(\"ORIGINAL: \"+ test_1_json[i]['speakers'][sentence_nr]+' // '+test_1_json[i]['utterances'][sentence_nr])\n",
        "\n",
        "      try:\n",
        "        translated_utterance = GoogleTranslator(source='hi', target='en').translate(test_1_json[i]['utterances'][sentence_nr])\n",
        "      except:\n",
        "        translated_utterance = test_1_json[i]['utterances'][sentence_nr]\n",
        "\n",
        "      current_sample = test_1_json[i]['speakers'][sentence_nr]+' // ' + translated_utterance + ' // '\n",
        "\n",
        "      prompt = \"\\n Use the CONTEXT to complete the SENTENCE using ONLY one emotion among: disgust, joy, neutral, anger, sadness, contempt, surprise, fear. Do not explain!\\n\"\n",
        "      prompt += f\"<s>[INST] CONTEXT: {few_shot_samples}\\n SENTENCE:{current_sample} [/INST]\"\n",
        "\n",
        "      #print(prompt)\n",
        "\n",
        "      response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.5,\n",
        "        logprobs=1,\n",
        "      )\n",
        "\n",
        "      answer = str(response[\"choices\"][0][\"text\"]).strip().lower()\n",
        "      answer = answer.split()[0]\n",
        "      # Sometime output contains a '.' remove it!\n",
        "      answer = answer.replace('.','')\n",
        "\n",
        "      # If the predicted word is not in emotion list just replace with neutral.\n",
        "      if answer not in emotions_list:\n",
        "        answer = 'neutral'\n",
        "\n",
        "      #current_sample += answer + \" \\n \"\n",
        "\n",
        "      print(\"GENERATED: \"+ current_sample+answer)\n",
        "\n",
        "      with open('answer.txt', 'a') as f:\n",
        "        f.write(answer+\"\\n\")\n",
        "\n",
        "break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results.zip result.json"
      ],
      "metadata": {
        "id": "JAUcpxAdivmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ea59ae-ddbf-4d20-80b8-dc4c5aa2159f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: result.json (deflated 94%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('results.zip')"
      ],
      "metadata": {
        "id": "dia3xJw7TwSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3de1381-cafc-4c99-ead9-56fe93650802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa9de8ad-0494-4c9b-aa6e-310112392859\", \"results.zip\", 3945)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fb88c9249204a1e8821a780a0405c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d93c51d56c4049fc92d3498897bd3f9b",
              "IPY_MODEL_caf10d9705ed4f6e9823a2356024c4c5",
              "IPY_MODEL_38fbcd9f2a524d90852171c10bf65917"
            ],
            "layout": "IPY_MODEL_7d5d5e1b599540a78dc76f5a1266517f"
          }
        },
        "d93c51d56c4049fc92d3498897bd3f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a07c423b64f249dda5b7e1df60fac339",
            "placeholder": "​",
            "style": "IPY_MODEL_78d38b363f7c46ac94814956d7565a03",
            "value": "  0%"
          }
        },
        "caf10d9705ed4f6e9823a2356024c4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7edc3eef71e451883d3584f8c2f6215",
            "max": 343,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22e25257237d4f7e994384fc0e6788c6",
            "value": 1
          }
        },
        "38fbcd9f2a524d90852171c10bf65917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0edd43dd83ee4731873add73e5c2039a",
            "placeholder": "​",
            "style": "IPY_MODEL_f94e6f30c89c412aa840139f285ea591",
            "value": " 1/343 [00:03&lt;21:55,  3.85s/it]"
          }
        },
        "7d5d5e1b599540a78dc76f5a1266517f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07c423b64f249dda5b7e1df60fac339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d38b363f7c46ac94814956d7565a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7edc3eef71e451883d3584f8c2f6215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e25257237d4f7e994384fc0e6788c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0edd43dd83ee4731873add73e5c2039a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94e6f30c89c412aa840139f285ea591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52280b866d7245d99f5891e4a73e146b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a784104413cf4a6aa1c8b51ba6e99fea",
              "IPY_MODEL_f168f7ac54f44ead9fd3d1baa23316f6",
              "IPY_MODEL_415bfee5ea7645e3864005ee2f40eb95"
            ],
            "layout": "IPY_MODEL_551493ce6e0d484a86bf4a481ecc1bf6"
          }
        },
        "a784104413cf4a6aa1c8b51ba6e99fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6107ef20cf304c0f8d72b125f7e35400",
            "placeholder": "​",
            "style": "IPY_MODEL_86b3b3e6aaae45a4b374223573bc89b5",
            "value": "  4%"
          }
        },
        "f168f7ac54f44ead9fd3d1baa23316f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1317cd19a340452fa46052e337fc57a4",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c810d2e07474cfba53833152a45dff2",
            "value": 2
          }
        },
        "415bfee5ea7645e3864005ee2f40eb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2d59f99dc52485ca69fc6a9526e2909",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4978f75aa446dfb19328a6a2d5c939",
            "value": " 2/46 [00:03&lt;01:09,  1.58s/it]"
          }
        },
        "551493ce6e0d484a86bf4a481ecc1bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6107ef20cf304c0f8d72b125f7e35400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b3b3e6aaae45a4b374223573bc89b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1317cd19a340452fa46052e337fc57a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c810d2e07474cfba53833152a45dff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2d59f99dc52485ca69fc6a9526e2909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4978f75aa446dfb19328a6a2d5c939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b39542d0d94fef85de4ee76f15253b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c53c6e842534842800f5bf4a40cede6",
              "IPY_MODEL_30ff0df7708d40a4b31403a62e3275b2",
              "IPY_MODEL_269eeb288ec14bfb8783b27453a66984"
            ],
            "layout": "IPY_MODEL_0a739823114e4d2a9fe788a092c0f8dc"
          }
        },
        "7c53c6e842534842800f5bf4a40cede6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1432c20d2224906b2f58eb3c7ad03a6",
            "placeholder": "​",
            "style": "IPY_MODEL_c552d29fc9dc4559a7e94b89929a9f49",
            "value": "100%"
          }
        },
        "30ff0df7708d40a4b31403a62e3275b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6372844a138e499885e16433e6a1a54d",
            "max": 343,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1fccf6a12a04fae8200babfc160f66a",
            "value": 343
          }
        },
        "269eeb288ec14bfb8783b27453a66984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0212db505ad74b17a3dcfa40eb46a1a4",
            "placeholder": "​",
            "style": "IPY_MODEL_597637adf57c456780b6d55460ef797c",
            "value": " 343/343 [00:00&lt;00:00, 16146.60it/s]"
          }
        },
        "0a739823114e4d2a9fe788a092c0f8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1432c20d2224906b2f58eb3c7ad03a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c552d29fc9dc4559a7e94b89929a9f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6372844a138e499885e16433e6a1a54d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1fccf6a12a04fae8200babfc160f66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0212db505ad74b17a3dcfa40eb46a1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "597637adf57c456780b6d55460ef797c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56426e3783e84b3a9ff130e2961e0dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_671d225527d4482f9082c0e69edba200",
              "IPY_MODEL_76db46c7390045fcbd28259f483462ce",
              "IPY_MODEL_301194cc1abf4b0f8f699e3b9b442a30"
            ],
            "layout": "IPY_MODEL_d57c43b2ea88417a99854d06a3b7831a"
          }
        },
        "671d225527d4482f9082c0e69edba200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b288cba450b94209bb56c6a47fcae8bb",
            "placeholder": "​",
            "style": "IPY_MODEL_2bd368e3dbd8437c984df36a43dc2a23",
            "value": "100%"
          }
        },
        "76db46c7390045fcbd28259f483462ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff1289a5acc4786930ac433617c370d",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c101ef0afd1742daa4a9a0e7043dd80c",
            "value": 46
          }
        },
        "301194cc1abf4b0f8f699e3b9b442a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccc99f43992e45b8b69f095cddaa4965",
            "placeholder": "​",
            "style": "IPY_MODEL_0f12db3e94724f39a1afbb021f909751",
            "value": " 46/46 [00:00&lt;00:00, 1861.15it/s]"
          }
        },
        "d57c43b2ea88417a99854d06a3b7831a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b288cba450b94209bb56c6a47fcae8bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd368e3dbd8437c984df36a43dc2a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff1289a5acc4786930ac433617c370d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c101ef0afd1742daa4a9a0e7043dd80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccc99f43992e45b8b69f095cddaa4965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f12db3e94724f39a1afbb021f909751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}